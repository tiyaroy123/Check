{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiyaroy123/Check/blob/main/Calculate_Distance_between_two_wiki_pages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSFf6IglCuy-",
        "outputId": "ecec8ca7-e386-4945-b7a1-00f67f84f480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cache... Cache loaded in 0.00 seconds.\n",
            "save cache on shutdown? [y/n]: y\n",
            "starting page: https://en.wikipedia.org/wiki/Upper_house\n",
            "destination page: https://en.wikipedia.org/wiki/Citizenship\n",
            "Depth (integer value): 8\n",
            "depth too high, this is going to take too long!!!\n",
            "checking 0 links on level 8\n",
            "depth too high, this is going to take too long!!!\n",
            "checking 1 links on level 8\n",
            "Upper_house\n",
            "(cached) Upper_house\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "depth too high, this is going to take too long!!!\n",
            "checking 82 links on level 8\n",
            "Upper_house -> Sudan\n",
            "Upper_house -> United Kingdom\n",
            "Upper_house -> Prussian House of Lords\n",
            "Upper_house -> Austrian House of Lords\n",
            "Upper_house -> ex officio\n",
            "Upper_house -> Austria\n",
            "Upper_house -> Russia\n",
            "Upper_house -> Committee\n",
            "Upper_house -> International parliament\n",
            "Upper_house -> New Zealand\n",
            "Upper_house -> Canada\n",
            "Upper_house -> Turkey\n",
            "\n",
            "****** Solution Found ******\n",
            "Upper_house -> Turkey -> citizen\n",
            "show result? [y/n] y\n",
            "[('Upper_house', 'Upper_house'), ('Turkey', 'Turkey'), ('Citizenship', 'citizen')]\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "import json\n",
        "from pathlib import Path\n",
        "from queue import Queue\n",
        "import re\n",
        "from time import time, sleep\n",
        "from threading import Thread\n",
        "import sys\n",
        "\n",
        "\n",
        "CACHE_FILE = \"cache.json\"\n",
        "\n",
        "\n",
        "class Page:\n",
        "    def __init__(self, url, title, parent, depth=0):\n",
        "        self.url = url\n",
        "        self.title = title\n",
        "        self.parent = parent\n",
        "        self.depth = depth\n",
        "\n",
        "\n",
        "def strip_url(url):\n",
        "    return re.search(r\"\\/wiki\\/(.+)$\", url).group(1)\n",
        "\n",
        "\n",
        "def link_is_valid(link):\n",
        "    if link.get('href') and link.get('href')[:6] == \"/wiki/\":\n",
        "        if (link.contents and str(link.contents[0])[0] != \"<\"\n",
        "                and \":\" not in link.get('href')):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def get_connection(page, connections):\n",
        "    connections.append((page, urllib.request.urlopen(\n",
        "        \"https://en.wikipedia.org/wiki/\" + page.url)))\n",
        "\n",
        "\n",
        "def get_links_from_page(page, connection):\n",
        "    print(\" -> \".join([r[1] for r in build_path(page)]))\n",
        "    links = []\n",
        "    soup = BeautifulSoup(connection, \"lxml\").find(\n",
        "        \"div\", {\"id\": \"mw-content-text\"})\n",
        "    # exlude \"references\" section\n",
        "    for div in soup.find_all(\"div\", {'class': 'reflist'}):\n",
        "        div.decompose()\n",
        "    for div in soup.find_all(\"div\", {'class': 'navbox'}):\n",
        "        div.decompose()\n",
        "    for div in soup.find_all(\"div\", {'class': 'refbegin'}):\n",
        "        div.decompose()\n",
        "    for paragraph in soup.findAll('p'):\n",
        "        for link in paragraph.findAll('a'):\n",
        "            if link_is_valid(link):\n",
        "                links.append(link)\n",
        "    for list in soup.findAll('ul'):\n",
        "        for link in list.findAll('a'):\n",
        "            if link_is_valid(link):\n",
        "                links.append(link)\n",
        "    return [(a.get('href')[6:], a.contents[0]) for a in links]\n",
        "\n",
        "\n",
        "def load_cache():\n",
        "    print(\"Loading cache... \", end=\"\")\n",
        "    sys.stdout.flush()\n",
        "    start_time = time()\n",
        "    table_file = Path(CACHE_FILE)\n",
        "    if table_file.exists():\n",
        "        cache = json.load(open(CACHE_FILE))\n",
        "    else:\n",
        "        cache = dict()\n",
        "    print(\"Cache loaded in {:.2f} seconds.\".format(time() - start_time))\n",
        "    return cache\n",
        "\n",
        "\n",
        "def write_cache(table):\n",
        "    print(\"Updating cache... \", end=\"\")\n",
        "    sys.stdout.flush()\n",
        "    with open(CACHE_FILE, 'w') as file:\n",
        "        file.write(json.dumps(table))\n",
        "    print(\"Done\")\n",
        "\n",
        "\n",
        "def build_path(current):\n",
        "    path = []\n",
        "    while current.parent:\n",
        "        path.append((current.url, current.title))\n",
        "        current = current.parent\n",
        "    path.append((current.url, current.title))\n",
        "    path.reverse()\n",
        "    return path\n",
        "\n",
        "\n",
        "def check_links(links, visited, queue, goal_suffix, page, grow_cache):\n",
        "    for url, title in links:\n",
        "        if url not in visited:\n",
        "            if url == goal_suffix:\n",
        "                if grow_cache:\n",
        "                    write_cache(cache)\n",
        "                p = build_path(page)\n",
        "                p.append((url, title))\n",
        "                return p\n",
        "            visited.add(url)\n",
        "            queue.put(Page(url, title, page, page.depth + 1))\n",
        "    return None\n",
        "\n",
        "\n",
        "def relate(start, destination, current_depth, depth, grow_cache=False):\n",
        "    visited = set()\n",
        "    q = Queue()\n",
        "    # add initial state with no parent\n",
        "    q.put(Page(strip_url(start), strip_url(start), None))\n",
        "    goal_suffix = strip_url(destination)\n",
        "    # current_depth = depth or  0\n",
        "    web_links = []\n",
        "    while True:\n",
        "        if current_depth > 7:\n",
        "            print(\"depth too high, this is going to take too long!!!\")\n",
        "\n",
        "            # return \"None\"\n",
        "        if not q.empty():\n",
        "            page = q.get()\n",
        "\n",
        "        if q.empty() or page.depth > current_depth:\n",
        "            if len(web_links) > 2000:\n",
        "                print(\"too many web links ({} at level {}) \".format(\n",
        "                    len(web_links), current_depth))\n",
        "                if grow_cache:\n",
        "                    write_cache(cache)\n",
        "                return None\n",
        "            print(\"checking {} links on level {}\".format(\n",
        "                len(web_links), current_depth))\n",
        "            if not q.empty():\n",
        "                current_depth += 1\n",
        "\n",
        "            while web_links:\n",
        "                connections = []\n",
        "                threads = []\n",
        "                for i in range(100):\n",
        "                    if web_links:\n",
        "                        t = Thread(target=get_connection, args=(\n",
        "                            web_links.pop(), connections))\n",
        "                        threads.append(t)\n",
        "                        t.start()\n",
        "                for i in range(len(threads)):\n",
        "                    threads[i].join()\n",
        "                for connection in connections:\n",
        "                    links = get_links_from_page(connection[0], connection[1])\n",
        "                    cache[connection[0].url] = links\n",
        "                    result = check_links(\n",
        "                        links, visited, q, goal_suffix, connection[0], grow_cache)\n",
        "                    if result:\n",
        "                        return result\n",
        "                if web_links:\n",
        "                    print(\"{} links left to check...\".format(len(web_links)))\n",
        "\n",
        "            web_links = []\n",
        "\n",
        "        if page.url in cache:\n",
        "            print(\"(cached) \" + \" -> \".join([r[1] for r in build_path(page)]))\n",
        "            links = cache[page.url]\n",
        "        else:\n",
        "            # print(\" -> \".join(build_path(page)))\n",
        "            web_links.append(page)\n",
        "            continue\n",
        "        result = check_links(links, visited, q, goal_suffix, page, grow_cache)\n",
        "        if result:\n",
        "            return result\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cache = load_cache()\n",
        "    from os import system\n",
        "    import requests\n",
        "    grow_cache = input(\n",
        "        \"save cache on shutdown? [y/n]: \").strip().upper() == \"Y\"\n",
        "    while True:\n",
        "        try:\n",
        "            start_link = input(\"starting page: \")\n",
        "            dest_link = input(\"destination page: \")\n",
        "            current_depth = int(input(\"Depth (integer value): \"))\n",
        "            result = relate(start_link, dest_link,  current_depth, cache, grow_cache=False)\n",
        "            if result:\n",
        "                print(\"\\n****** Solution Found ******\")\n",
        "                print(\" -> \".join([r[1] for r in result]))\n",
        "                if input(\"show result? [y/n] \").upper().strip() == \"Y\":\n",
        "                    print(result)\n",
        "                    # pass\n",
        "            else:\n",
        "                print(\"Solution not found :(\")\n",
        "        except (KeyboardInterrupt) as e:\n",
        "            print(e)\n",
        "            if grow_cache:\n",
        "                write_cache(cache)\n",
        "            break"
      ]
    }
  ]
}